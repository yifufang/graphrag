### This config file contains required core defaults that must be set, along with a handful of common optional settings.
### For a full list of available settings, see https://microsoft.github.io/graphrag/config/yaml/

### LLM settings ###
## There are a number of settings to tune the threading and token limits for LLM calls - check the docs.

models:
  default_chat_model:
    type: openai_chat # using openai_chat type for vllm compatibility
    api_base: http://43.143.178.100:8000/model3/v1  # 您的LLM模型端点
    api_key: Mj68hxjGAt4y%N1tf@,A@@BzD1      # 您的vllm api key
    model: DeepSeek-R1-Distill-Qwen-32B      # 与您部署的模型名称一致
    encoding_model: cl100k_base              # explicit tokenizer for custom model
    model_supports_json: true
    max_tokens: 4000              # 增加到4000以适应中文和复杂的JSON输出
    temperature: 0.1              # 稍微提高温度以获得更多样的中文表达
    concurrent_requests: 6       
    async_mode: threaded
    retry_strategy: native
    max_retries: 5                # 增加重试次数
    tokens_per_minute: 50000     # 大幅增加，匹配更强的处理能力
    requests_per_minute: 100      # 大幅增加请求频率
  default_embedding_model:
    type: openai_embedding # 您的embedding模型
    api_base: http://43.143.178.100:8000/model5/v1  # 您的embedding模型端点
    api_key: Mj68hxjGAt4y%N1tf@,A@@BzD1      
    model: bge-m3  # 您的embedding模型名称
    encoding_model: cl100k_base              
    concurrent_requests: 8    # 适当增加嵌入模型并发
    request_timeout: 60       # 增加超时时间
    async_mode: threaded
    retry_strategy: native
    max_retries: 5            # 增加重试次数
    tokens_per_minute: 100000  # 增加嵌入模型处理能力
    requests_per_minute: 200   # 增加请求频率

### Input settings ###

input:
  storage:
    type: file
    base_dir: "input"
  file_type: text # 支持文本文件类型
  file_pattern: ".*\\.(txt|md)$$"  # 支持 .txt 和 .md 文件，使用$$转义$$符号
  encoding: utf-8  # 确保正确处理中文编码

chunks:
  size: 800         # 适当减小chunk大小，因为中文字符密度更高
  overlap: 80       # 相应调整overlap为chunk大小的10%
  group_by_columns: [id]

### Output/storage settings ###
## Using local file storage

output:
  type: file
  base_dir: "output"
    
cache:
  type: file
  base_dir: "cache"

reporting:
  type: file
  base_dir: "logs"

vector_store:  # 重新启用向量存储
  default_vector_store:
    type: lancedb
    db_uri: output\lancedb
    container_name: default
    overwrite: True

### Workflow settings ###

embed_text:
  model_id: default_embedding_model  # 重新启用文本嵌入
  vector_store_id: default_vector_store
  batch_max_tokens: 6000             # 适当降低以适应中文token计算
  batch_size: 12                     # 适当降低批处理大小

extract_graph:
  model_id: default_chat_model
  prompt: "prompts/extract_graph.txt"
  entity_types: [药材,方剂,疾病,症状,医家,功效]  # 去掉医书，专注于6个核心中医实体
  max_gleanings: 2              # 增加到2次迭代，提高中医实体关系提取的完整性

summarize_descriptions:
  model_id: default_chat_model
  prompt: "prompts/summarize_descriptions.txt"
  max_length: 300               # 增加长度以适应中文表达

extract_graph_nlp:
  text_analyzer:
    extractor_type: regex_english  # 注意：这里可能需要针对中文调整

cluster_graph:
  max_cluster_size: 10          # 适当增加聚类大小以适应中医知识的复杂性

extract_claims:
  enabled: true                  # 开启声明提取，对中医功效验证很有价值
  model_id: default_chat_model
  prompt: "prompts/extract_claims.txt"
  description: "与药材功效、方剂应用、疾病治疗相关的声明，包括现代研究验证和传统文献记载。"
  max_gleanings: 1

community_reports:
  model_id: default_chat_model
  graph_prompt: "prompts/community_report_graph.txt"
  text_prompt: "prompts/community_report_text.txt"
  max_length: 1200              # 增加长度以适应中文社区报告
  max_input_length: 3000       # 增加输入长度

embed_graph:
  enabled: false

umap:
  enabled: false

snapshots:
  graphml: false
  embeddings: false

### Query settings ###

local_search:
  chat_model_id: default_chat_model
  embedding_model_id: default_embedding_model  # 重新启用
  prompt: "prompts/local_search_system_prompt.txt"

global_search:
  chat_model_id: default_chat_model
  map_prompt: "prompts/global_search_map_system_prompt.txt"
  reduce_prompt: "prompts/global_search_reduce_system_prompt.txt"
  knowledge_prompt: "prompts/global_search_knowledge_system_prompt.txt"

drift_search:
  chat_model_id: default_chat_model
  embedding_model_id: default_embedding_model  # 重新启用
  prompt: "prompts/drift_search_system_prompt.txt"
  reduce_prompt: "prompts/drift_search_reduce_prompt.txt"

basic_search:
  chat_model_id: default_chat_model
  embedding_model_id: default_embedding_model  # 重新启用
  prompt: "prompts/basic_search_system_prompt.txt"
