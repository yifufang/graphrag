### This config file contains required core defaults that must be set, along with a handful of common optional settings.
### For a full list of available settings, see https://microsoft.github.io/graphrag/config/yaml/

### LLM settings ###
## There are a number of settings to tune the threading and token limits for LLM calls - check the docs.

models:
  default_chat_model:
    type: openai_chat # using openai_chat type for vllm compatibility
    api_base: http://43.143.178.100:8000/model3/v1  # 您的LLM模型端点
    api_key: Mj68hxjGAt4y%N1tf@,A@@BzD1      # 您的vllm api key
    model: DeepSeek-R1-Distill-Qwen-32B      # 与您部署的模型名称一致
    encoding_model: cl100k_base              # explicit tokenizer for custom model
    model_supports_json: true
    max_tokens: 6000              # 16k context支持更长输出，提高中医描述质量
    temperature: 0.1              # 稍微提高温度以获得更多样的中文表达
    concurrent_requests: 20       
    async_mode: threaded
    retry_strategy: native
    max_retries: 5                # 增加重试次数
    tokens_per_minute: 300000     # 大幅增加，匹配更强的处理能力
    requests_per_minute: 300      # 大幅增加请求频率
  default_embedding_model:
    type: openai_embedding # 您的embedding模型
    api_base: http://43.143.178.100:8000/model5/v1  # 您的embedding模型端点
    api_key: Mj68hxjGAt4y%N1tf@,A@@BzD1      
    model: bge-m3  # 您的embedding模型名称
    encoding_model: cl100k_base              
    concurrent_requests: 24    # BGE-M3性能优异，进一步增加并发
    request_timeout: 45       # 适当增加超时，因为批次更大
    async_mode: threaded
    retry_strategy: native
    max_retries: 5            # 增加重试次数
    tokens_per_minute: 300000  # 充分利用BGE-M3的处理能力
    requests_per_minute: 500   # 进一步增加请求频率

### Input settings ###

input:
  storage:
    type: file
    base_dir: "input"
  file_type: text # 支持文本文件类型
  file_pattern: ".*\\.(txt|md)$$"  # 支持 .txt 和 .md 文件，使用$$转义$$符号
  encoding: utf-8  # 确保正确处理中文编码

chunks:
  size: 1200        # 利用16k context，增大chunk以保持更多上下文
  overlap: 120      # 相应调整overlap为chunk大小的10%
  group_by_columns: [id]

### Output/storage settings ###
## Using local file storage

output:
  type: file
  base_dir: "output"
    
cache:
  type: file
  base_dir: "cache"

reporting:
  type: file
  base_dir: "logs"

vector_store:  # 重新启用向量存储
  default_vector_store:
    type: lancedb
    db_uri: output/lancedb
    container_name: default
    overwrite: True

### Workflow settings ###

embed_text:
  model_id: default_embedding_model  # 重新启用文本嵌入
  vector_store_id: default_vector_store
  batch_max_tokens: 7500             # 充分利用BGE-M3的8192上下文，留500token余量
  batch_size: 32                     # 增加批处理，提高embedding效率

extract_graph:
  model_id: default_chat_model
  prompt: "prompts/extract_graph.txt"
  entity_types: [脉象,脉位,病症,治疗方法,医家,著作,临床案例,脉诊系统]  # 飞龙脉诊核心实体类型，按重要性排序
  max_gleanings: 1              # 保持1次迭代，确保提取质量

summarize_descriptions:
  model_id: default_chat_model
  prompt: "prompts/summarize_descriptions.txt"
  max_length: 500               # 利用16k context，生成更详细的描述

# extract_graph_nlp:  # 标准方法不需要这个配置，完全依赖LLM提取
  # text_analyzer:
    # extractor_type: regex_english

cluster_graph:
  max_cluster_size: 10          # 适当增加聚类大小以适应中医知识的复杂性

extract_claims:
  enabled: true                  # 开启声明提取，对飞龙脉诊验证很有价值
  model_id: default_chat_model
  prompt: "prompts/extract_claims.txt"
  description: "与脉象诊断、脉位对应、病症判断、治疗方法相关的声明，包括临床验证和传统文献记载。"
  max_gleanings: 1

community_reports:
  model_id: default_chat_model
  graph_prompt: "prompts/community_report_graph.txt"
  text_prompt: "prompts/community_report_text.txt"
  max_length: 1500              # 利用16k context，增加输出质量
  max_input_length: 5000       # 大幅增加输入长度，充分利用16k context

embed_graph:
  enabled: false

umap:
  enabled: false

snapshots:
  graphml: false
  embeddings: false

### Query settings ###

local_search:
  chat_model_id: default_chat_model
  embedding_model_id: default_embedding_model  # 重新启用
  prompt: "prompts/local_search_system_prompt.txt"

global_search:
  chat_model_id: default_chat_model
  map_prompt: "prompts/global_search_map_system_prompt.txt"
  reduce_prompt: "prompts/global_search_reduce_system_prompt.txt"
  knowledge_prompt: "prompts/global_search_knowledge_system_prompt.txt"

drift_search:
  chat_model_id: default_chat_model
  embedding_model_id: default_embedding_model  # 重新启用
  prompt: "prompts/drift_search_system_prompt.txt"
  reduce_prompt: "prompts/drift_search_reduce_prompt.txt"

basic_search:
  chat_model_id: default_chat_model
  embedding_model_id: default_embedding_model  # 重新启用
  prompt: "prompts/basic_search_system_prompt.txt"
